{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: wiki40b/en\n",
      "INFO:absl:Load dataset info from gs://tfds-data/datasets/wiki40b/en/1.3.0\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset wiki40b (gs://tfds-data/datasets/wiki40b/en/1.3.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train[:5], from gs://tfds-data/datasets/wiki40b/en/1.3.0\n"
     ]
    }
   ],
   "source": [
    "data = du.get_wiki_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for d in data:\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        proced_text = du.preproc_text(para)\n",
    "        if len(proced_text) < 500:\n",
    "            continue\n",
    "        paragraphs.append(proced_text)\n",
    "        # print(proced_text)\n",
    "        # print(len(proced_text))\n",
    "        \n",
    "        #print(\"~\"*20)\n",
    "    #print(\"#\"*20)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format\n",
    "* We need to keep both paragraph level and document level examples together.\n",
    "* We can train consecutively any paragraph but also we can train any document with multiple paragraphs\n",
    "\n",
    "## First try\n",
    "* Split by paragraph\n",
    "* Filter for paragraph length (above some length)\n",
    "* We get roughly 1 paragraph over 1000 characters per document. That's not great.\n",
    "* 789 / 500 docs with cutoff at 500 characters\n",
    "\n",
    "# TODO\n",
    "* Upgrade Spacy en_core_web_sm to latest\n",
    "* fix data encoder error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = du.encode_data_srl_only(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_repr(parse, word2veclist):\n",
    "    unique_tags = set()\n",
    "    sent_repr = []\n",
    "    for verb in parse:\n",
    "        tag_2_vec = defaultdict(list)\n",
    "        assert len(verb['tags']) == len(word2veclist.keys())\n",
    "        for i, tag in enumerate(verb['tags']):\n",
    "            if tag == 'O' or tag[2] == \"R\" or tag[2] == \"C\":\n",
    "                continue\n",
    "            unique_tags.add(tag[2:])\n",
    "            for vec_index in word2veclist[i]:\n",
    "                tag_2_vec[tag.split(\"-\")[1]].append(vec_index)\n",
    "        for key in tag_2_vec.keys():\n",
    "            assert key in du.TAGS, key +  str(list(unique_tags))\n",
    "        verb_repr = []\n",
    "        for tag in du.TAGS:\n",
    "            if tag not in tag_2_vec:\n",
    "                continue\n",
    "            vecs = tag_2_vec[tag]\n",
    "            verb_repr.append((tag, vecs))\n",
    "        sent_repr.append(verb_repr)\n",
    "    return sent_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_srl_only(data):\n",
    "    nlp = du.create_nlp()\n",
    "    tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    predictor = du.make_allennlp()\n",
    "    reprs = []\n",
    "    for ex_idx, d in enumerate(data):\n",
    "        text = d['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8')\n",
    "        for para in text.split(\"_START_\"):\n",
    "            if not para.startswith(\"PARAGRAPH_\"):\n",
    "                continue\n",
    "            example = du.preproc_text(para)\n",
    "            if len(example) < 500:\n",
    "                continue\n",
    "            doc = nlp(example)\n",
    "            for sent in doc.sents:\n",
    "                span = sent.text\n",
    "                lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "                srl_parse = predictor.predict(sentence=span)\n",
    "                token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "                try:\n",
    "                    word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)\n",
    "                except du.NoStopException as e:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(\"Index\", ex_idx)\n",
    "                    print(e)\n",
    "                    print(token_strs)\n",
    "                    print(srl_parse['words'])\n",
    "                    raise e\n",
    "                sent_repr = build_repr(srl_parse['verbs'], word2veclist)\n",
    "                reprs.append((span, sent_repr))\n",
    "    return reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.', [[('V', [2])], [('V', [3]), ('ARG1', [1]), ('ARGM', [4, 5, 6, 7, 8])], [('V', [12])], [('V', [13]), ('ARG1', [11]), ('ARGM', [14, 15, 16, 17])]]), ('His parents, Mr. and Mrs. William Gill Clayton, were Goldwater Republicans.', [[('V', [13]), ('ARG1', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]), ('ARG2', [14, 15, 16])]]), ('His father lobbied Congress on religious liberty issues.', [[('V', [3]), ('ARG0', [1, 2]), ('ARG1', [5, 6, 7, 8]), ('ARG2', [4])]]), ('Clayton graduated from high school and served in the United States Army Reserve.', [[('V', [2]), ('ARG0', [1]), ('ARG1', [3, 4, 5])], [('V', [7]), ('ARG0', [1]), ('ARG1', [8, 9, 10, 11, 12, 13])]]), ('He studied to be an aircraft electrician before he enrolled at Pensacola Christian College in Pensacola, Florida, from which he graduated in 2002.', [[('V', [2]), ('ARG0', [1]), ('ARG1', [3, 4, 5, 6, 7, 8]), ('ARGM', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])], [('V', [4]), ('ARG1', [1]), ('ARG2', [5, 6, 7, 8])], [('V', [11]), ('ARG0', [10]), ('ARG2', [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])], [('V', [24]), ('ARG0', [23]), ('ARG1', [21, 22]), ('ARGM', [25, 26])]]), ('\\nIn 2003, Clayton moved to Nashville.', [[('V', [5]), ('ARG1', [4]), ('ARG2', [6, 7]), ('ARGM', [1, 2])]]), ('His father died in 2004 and Clayton purchased a 1920s-era farmhouse on three acres in Whites Creek in suburban Davidson County.', [[('V', [3]), ('ARG1', [1, 2]), ('ARGM', [4, 5])], [('V', [8]), ('ARG0', [7]), ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])]]), ('Clayton, who has never been married, lives in Whites Creek with his dog, Saint.', [[('V', [4])], [('V', [6])], [('V', [7]), ('ARG1', [1]), ('ARGM', [5])], [('V', [9]), ('ARG0', [1, 2, 3, 4, 5, 6, 7, 8]), ('ARGM', [10, 11, 12, 13, 14, 15, 16, 17])]]), ('\\nClayton has worked at numerous jobs, including Target, a call center, as a floor installer, and as a salesperson of insurance, siding, and roofs.', [[('V', [2])], [('V', [3]), ('ARG0', [1]), ('ARG1', [15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]), ('ARGM', [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 20, 21])], [('V', [8]), ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]), ('ARG2', [5, 6])]]), ('He is a church youth group leader.', [[('V', [2]), ('ARG1', [1]), ('ARG2', [3, 4, 5, 6, 7])]]), ('He is currently employed with a moving company.', [[('V', [2])], [('V', [4]), ('ARG1', [1]), ('ARG2', [5, 6, 7, 8]), ('ARGM', [3])], [('V', [7]), ('ARG0', [8])]]), ('\\n', []), (\"\\nAfter Clayton's primary triumph, the Tennessee Democratic Party disavowed his candidacy and his vice-presidency of the socially conservative interest group, the Public Advocate of the United States, based in Washington, D.C. The group has been labelled a hate group by the Southern Poverty Law Center for its anti-gay rhetoric.\", [[('V', [12, 13, 14, 15]), ('ARG0', [8, 9, 10, 11]), ('ARG1', [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]), ('ARGM', [1, 2, 3, 4, 5, 6])], [('V', [38]), ('ARG1', [30, 31, 32, 33, 34, 35, 36]), ('ARGM', [39, 40, 41, 42, 43])], [('V', [48])], [('V', [49])], [('V', [50]), ('ARG0', [54, 55, 56, 57, 58, 59]), ('ARG1', [44, 45, 46, 47]), ('ARG2', [51, 52, 53]), ('ARGM', [60, 61, 62, 63, 64, 65])]]), ('\\nThe Tennessee Democratic Party issued this statement:\\nThe only time that Clayton has voted in a Democratic primary was when he was voting for himself.', [[('V', [5]), ('ARG0', [1, 2, 3, 4]), ('ARG1', [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])], [('V', [14])], [('V', [15]), ('ARG0', [13]), ('ARGM', [9, 10, 11, 16, 17, 18, 19])], [('V', [20]), ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]), ('ARG2', [21, 22, 23, 24, 25, 26])], [('V', [23])], [('V', [24]), ('ARG0', [22]), ('ARG1', [25, 26]), ('ARGM', [21])]]), ('Many Democrats in Tennessee knew nothing about any of the candidates in the race; so they voted for the person at the top of the ticket.', [[('V', [5]), ('ARG0', [1, 2, 3, 4]), ('ARG1', [6]), ('ARG2', [7, 8, 9, 10, 11, 12, 13, 14])], [('V', [18]), ('ARG0', [17]), ('ARG1', [19, 20, 21, 22, 23, 24, 25, 26, 27]), ('ARGM', [16])]]), ('Unfortunately, none of the other Democratic candidates was able to run the race needed to gain statewide visibility or support.', [[('V', [9]), ('ARG1', [3, 4, 5, 6, 7, 8]), ('ARG2', [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]), ('ARGM', [1])], [('V', [12]), ('ARG0', [3, 4, 5, 6, 7, 8]), ('ARG1', [13, 14, 15, 16, 17, 18, 19, 20, 21])], [('V', [15]), ('ARG1', [13, 14]), ('ARGM', [16, 17, 18, 19, 20, 21])], [('V', [17]), ('ARG0', [3, 4, 5, 6, 7, 8]), ('ARG1', [18, 19, 20, 21])]]), ('Mark Clayton is associated with a known hate group in Washington, D.C., and the Tennessee Democratic Party disavows his candidacy, will not do anything to promote or support him in any way, and urges Democrats to write-in a candidate of their choice in November.', [[('V', [3])], [('V', [4]), ('ARG1', [1, 2]), ('ARG2', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14])], [('V', [7]), ('ARG1', [8, 9])], [('V', [23, 24, 25, 26]), ('ARG0', [19, 20, 21, 22]), ('ARG1', [27, 28])], [('V', [30]), ('ARG0', [19, 20, 21, 22]), ('ARG1', [33, 34, 35, 36, 37, 38, 39, 40, 41]), ('ARGM', [31])], [('V', [32]), ('ARG0', [19, 20, 21, 22]), ('ARG1', [33, 34, 35, 36, 37, 38, 39, 40, 41]), ('ARGM', [30, 31])], [('V', [35]), ('ARG1', [38]), ('ARGM', [39, 40, 41])], [('V', [37]), ('ARG0', [33]), ('ARG1', [38]), ('ARGM', [39, 40, 41])], [('V', [44]), ('ARG0', [1, 2, 19, 20, 21, 22]), ('ARG1', [45]), ('ARG2', [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56])], [('V', [47]), ('ARG0', [45]), ('ARG1', [50, 51, 52, 53, 54]), ('ARGM', [55, 56])]]), ('\\nClayton won the Democratic nomination with 30% of the vote, despite raising no money and having a website that was four years out of date.', [[('V', [2]), ('ARG0', [1]), ('ARG1', [3, 4, 5]), ('ARGM', [6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])], [('V', [14]), ('ARG0', [1]), ('ARG1', [15, 16])], [('V', [18]), ('ARG0', [1]), ('ARG1', [19, 20, 21, 22, 23, 24, 25, 26, 27])], [('V', [22]), ('ARG1', [19, 20]), ('ARG2', [23, 24, 25, 26, 27])]]), ('The next day Tennessee\\'s Democratic Party disavowed the candidate over his active role in the Public Advocate of the United States, which they described as a \"known hate group\".', [[('V', [9, 10, 11, 12]), ('ARG0', [4, 5, 6, 7, 8]), ('ARG1', [13, 14]), ('ARG2', [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]), ('ARGM', [1, 2, 3])], [('V', [30]), ('ARG0', [29]), ('ARG1', [20, 21, 22, 23, 24, 25, 26]), ('ARG2', [31, 32, 33, 34, 35, 36, 37])], [('V', [34]), ('ARG1', [35, 36])]]), ('They blamed his victory among candidates, for whom the TNDP provided little forums to become known, on the fact that his name appeared first on the ballot, and said they would do nothing to help his campaign, urging Democrats to vote for \"the write-in candidate of their choice\" in November.', [[('V', [2]), ('ARG0', [1]), ('ARG1', [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]), ('ARG2', [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])], [('V', [13]), ('ARG0', [10, 11, 12]), ('ARG1', [14, 15, 16, 17, 18])], [('V', [17]), ('ARG2', [18])], [('V', [18]), ('ARG1', [14, 15])], [('V', [26]), ('ARG1', [24, 25]), ('ARGM', [27, 28, 29, 30])], [('V', [33]), ('ARG0', [1]), ('ARG1', [34, 35, 36, 37, 38, 39, 40, 41]), ('ARGM', [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])], [('V', [35])], [('V', [36]), ('ARG0', [34]), ('ARG1', [37]), ('ARGM', [35, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])], [('V', [39]), ('ARG1', [40, 41])], [('V', [43]), ('ARG0', [1]), ('ARG1', [44]), ('ARG2', [45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])], [('V', [46]), ('ARG0', [44]), ('ARG1', [47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]), ('ARGM', [58, 59])], [('V', [50])]]), ('One of the Democratic candidates, Larry Crim, filed a petition seeking to offer the voters a new primary in which to select a Democratic Nominee based on Democratic Chair Chip Forrester permitting Clayton, a nondemocratic candidate, at the top of ballot to benefit a candidate Forrester recruited and improperly endorsed - Overall - who did not win the primary.', [[('V', [11]), ('ARG0', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), ('ARG1', [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [14]), ('ARG0', [12, 13]), ('ARG1', [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [16]), ('ARG0', [12, 13]), ('ARG1', [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARG3', [17, 18])], [('V', [25]), ('ARG0', [17, 18]), ('ARG1', [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARGM', [19, 20, 21])], [('V', [29]), ('ARG1', [26, 27, 28]), ('ARG2', [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [36]), ('ARG0', [31, 32, 33, 34, 35]), ('ARG1', [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARG2', [37, 38, 39, 40, 41, 42, 43, 44, 45])], [('V', [52]), ('ARG0', [37, 38, 39, 40, 41, 42, 43, 44, 45]), ('ARG1', [53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [57]), ('ARG0', [55, 56]), ('ARG1', [53, 54])], [('V', [61]), ('ARG0', [55, 56]), ('ARG1', [53, 54]), ('ARGM', [59, 60])], [('V', [66])], [('V', [68]), ('ARG0', [53, 54]), ('ARG1', [69, 70]), ('ARGM', [67])]]), ('Forrester then disavowed Clayton he had allowed on the ballot after he received the most votes.', [[('V', [4, 5, 6, 7]), ('ARG0', [1, 2]), ('ARG1', [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), ('ARGM', [3])], [('V', [10])], [('V', [11]), ('ARG0', [9]), ('ARGM', [12, 13, 14, 15, 16, 17, 18, 19, 20])], [('V', [17]), ('ARG0', [16]), ('ARG1', [18, 19, 20])]]), ('The background is that the TNDP placed little emphasis on the U.S. Senate race in 2012 to replace Corker.', [[('V', [3]), ('ARG1', [1, 2]), ('ARG2', [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])], [('V', [8]), ('ARG0', [5, 6, 7]), ('ARG1', [9, 10]), ('ARG2', [11, 12, 13, 14, 15, 16, 17, 18]), ('ARGM', [19, 20])], [('V', [22]), ('ARG0', [5, 6, 7]), ('ARG1', [23, 24])]]), (\"Treasurer and Financial benefactor of the TNDP Bill Freeman who was under Forrester actually contributed to Republican Bob Corker's campaign and was later removed from office, followed by Forrester's not seeking another term subsequent to the TNDP 2012 fiasco.\", [[('V', [12]), ('ARG1', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), ('ARG2', [13, 14, 15])], [('V', [17]), ('ARG0', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]), ('ARG2', [18, 19, 20, 21, 22, 23, 24, 25]), ('ARGM', [16])], [('V', [27])], [('V', [29]), ('ARG1', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]), ('ARG2', [30, 31]), ('ARGM', [28, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])], [('V', [33]), ('ARG1', [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])], [('V', [40]), ('ARG0', [35, 36, 37, 38]), ('ARG1', [41, 42, 43, 44, 45, 46, 47, 48, 49, 50]), ('ARGM', [39])]]), ('Crim filed a preliminary motion seeking a temporary restraining order against certification of the results until the merits of the case for a new primary could be decided.', [[('V', [3]), ('ARG0', [1, 2]), ('ARG1', [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])], [('V', [7]), ('ARG0', [4, 5, 6]), ('ARG1', [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])], [('V', [27])], [('V', [28])], [('V', [29]), ('ARG1', [18, 19, 20, 21, 22, 23, 24, 25, 26]), ('ARGM', [27])]]), ('Yet, after a judge denied the temporary restraining order Crim withdrew his petition stating at a news conference outside the Federal Courthouse that the costs of proceeding and the costs of a new primary to the Democratic Party, even if Crim won, would be overwhelming especially given the political realities the party leaders conducted and permitted to the detriment of any Democratic Nominee.', [[('V', [6]), ('ARG0', [4, 5]), ('ARG1', [7, 8, 9, 10])], [('V', [13]), ('ARG0', [11, 12]), ('ARG1', [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARGM', [1, 3, 4, 5, 6, 7, 8, 9, 10])], [('V', [16]), ('ARG0', [14, 15]), ('ARG1', [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARGM', [17, 18, 19, 20, 21, 22, 23, 24])], [('V', [46]), ('ARG0', [44, 45])], [('V', [48])], [('V', [49]), ('ARG1', [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]), ('ARG2', [50]), ('ARGM', [42, 43, 44, 45, 46, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [52]), ('ARG1', [53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70]), ('ARGM', [51])], [('V', [59]), ('ARG0', [56, 57, 58]), ('ARG1', [53, 54, 55]), ('ARGM', [62, 63, 64, 65, 66, 67, 68, 69, 70])], [('V', [61]), ('ARG0', [56, 57, 58]), ('ARG1', [53, 54, 55]), ('ARG2', [62, 63, 64, 65, 66, 67, 68, 69, 70])]]), ('Mr. Crim was subsequently elected Chair of Democrats United For Tennessee in 2012.', [[('V', [5])], [('V', [7]), ('ARG1', [1, 2, 3, 4]), ('ARG2', [8, 9, 10, 11, 12, 13]), ('ARGM', [6, 14, 15])]]), (\"\\nClayton's nomination has been compared to that of the previously unknown Alvin Greene in the Democratic primary in the 2010 Senate race in South Carolina.\", [[('V', [5])], [('V', [6])], [('V', [7]), ('ARG1', [1, 2, 3, 4]), ('ARG2', [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])]]), ('Greene was then handily defeated by the Republican Jim DeMint.', [[('V', [2])], [('V', [6]), ('ARG0', [7, 8, 9, 10, 11, 12]), ('ARG1', [1]), ('ARGM', [3, 4, 5])]]), ('\\n', []), (\"\\nClayton's work at the Public Advocate of the United States, a conservative group based in Washington, D.C., has come under scrutiny.\", [[('V', [17]), ('ARG1', [14, 15, 16]), ('ARGM', [18, 19, 20, 21, 22])], [('V', [24, 25, 26])], [('V', [27]), ('ARG1', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]), ('ARG2', [28, 29])]]), ('The group has been designated as an anti-gay hate group by the Alabama-based Southern Poverty Law Center.', [[('V', [3])], [('V', [4])], [('V', [5]), ('ARG0', [13, 14, 15, 16, 17, 18, 19, 20, 21]), ('ARG1', [1, 2]), ('ARG2', [6, 7, 8, 9, 10, 11, 12])], [('V', [17]), ('ARG1', [18, 19, 20, 21]), ('ARGM', [15])]]), ('\\nIn a press release The Public Advocate proclaimed that it \"associates with members of both major parties in a non-partisan fashion and promotes traditional values\".', [[('V', [8]), ('ARG0', [5, 6, 7]), ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]), ('ARGM', [1, 2, 3, 4])], [('V', [26]), ('ARG0', [10]), ('ARG1', [27, 28])]]), ('The organization contends that Clayton has demonstrated that \"an American patriot can put his or her name on the ballot and win big as a conservative, even in the Democratic Party.\"', [[('V', [3, 4]), ('ARG0', [1, 2]), ('ARG1', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])], [('V', [7])], [('V', [8]), ('ARG0', [6]), ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])], [('V', [14])], [('V', [15]), ('ARG0', [11, 12, 13]), ('ARG1', [16, 17, 18, 19]), ('ARG2', [20, 21, 22]), ('ARGM', [14])], [('V', [24]), ('ARG0', [11, 12, 13]), ('ARGM', [14, 25, 26, 27, 28, 30, 31, 32, 33, 34])]]), ('A Clayton spokesman criticized the state Democratic party for disowning the nominee and argued that the state party had violated the law by using its resources to attack one of its own candidates.', [[('V', [4]), ('ARG0', [1, 2, 3]), ('ARG1', [5, 6, 7, 8]), ('ARG2', [9, 10, 11, 12, 13, 14, 15])], [('V', [10, 11, 12, 13]), ('ARG0', [5, 6, 7, 8]), ('ARG1', [14, 15])], [('V', [17]), ('ARG0', [1, 2, 3]), ('ARG1', [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36])], [('V', [22])], [('V', [23]), ('ARG0', [19, 20, 21]), ('ARG1', [24, 25]), ('ARGM', [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36])], [('V', [27]), ('ARG0', [19, 20, 21]), ('ARG1', [28, 29]), ('ARG2', [30, 31, 32, 33, 34, 35, 36])], [('V', [31]), ('ARG0', [19, 20, 21]), ('ARG1', [32, 33, 34, 35, 36])]]), ('The Clayton campaign also said that it would file a complaint with the Federal Election Commission.', [[('V', [5]), ('ARG0', [1, 2, 3]), ('ARG1', [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]), ('ARGM', [4])], [('V', [8])], [('V', [9]), ('ARG0', [7]), ('ARG1', [10, 11]), ('ARG2', [12, 13, 14, 15, 16]), ('ARGM', [8])]])]\n"
     ]
    }
   ],
   "source": [
    "reprs = encode_data_srl_only(data[:2])\n",
    "print(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.',\n",
       " [[('V', [2])],\n",
       "  [('V', [3]), ('ARG1', [1]), ('ARGM', [4, 5, 6, 7, 8])],\n",
       "  [('V', [12])],\n",
       "  [('V', [13]), ('ARG1', [11]), ('ARGM', [14, 15, 16, 17])]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "span = '\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    "lm_tokenized_input = tokenizer(span, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11811,  2001,  2141,  1999,  4684,  1999,  2148,  6041,  1010,\n",
       "          2021,  2002,  2001, 23295,  1999, 10297,  1010,  3448,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix parsing errors\n",
    "* fix parser\n",
    "* update spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: wiki40b/en\n",
      "INFO:absl:Load dataset info from gs://tfds-data/datasets/wiki40b/en/1.3.0\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset wiki40b (gs://tfds-data/datasets/wiki40b/en/1.3.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train[:500], from gs://tfds-data/datasets/wiki40b/en/1.3.0\n"
     ]
    }
   ],
   "source": [
    "data = du.get_wiki_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-c646a8257b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_data_srl_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-166-5e4ddabd3e56>\u001b[0m in \u001b[0;36mencode_data_srl_only\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mlm_tokenized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0msrl_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mtoken_strs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm_tokenized_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msemantic\u001b[0m \u001b[0mroles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_tokenized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"verbs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/allennlp_models/structured_prediction/predictors/srl.py\u001b[0m in \u001b[0;36mpredict_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInstance\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"verbs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/allennlp_models/structured_prediction/models/srl_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, verb_indicator, metadata, tags)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverb_indicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 )\n\u001b[1;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/discoher/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reprs = encode_data_srl_only(data[27:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Other demolished features include a barnyard where the Marchienne-au-Pont municipal swimming pool now stands.',\n",
       " [[('V', [2]), ('ARG1', [3])],\n",
       "  [('V', [4]),\n",
       "   ('ARG1', [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]),\n",
       "   ('ARG2', [1, 2, 3])],\n",
       "  [('V', [21]),\n",
       "   ('ARG1', [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       "   ('ARG2', [5, 6, 7]),\n",
       "   ('ARGM', [20])]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprs[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = du.make_allennlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbs': [{'verb': 'was',\n",
       "   'description': 'Clayton [V: was] born in Mobile in south Alabama , but he was reared in Alexandria , Virginia .',\n",
       "   'tags': ['O',\n",
       "    'B-V',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'born',\n",
       "   'description': '[ARG1: Clayton] was [V: born] [ARGM-LOC: in Mobile in south Alabama] , but he was reared in Alexandria , Virginia .',\n",
       "   'tags': ['B-ARG1',\n",
       "    'O',\n",
       "    'B-V',\n",
       "    'B-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'was',\n",
       "   'description': 'Clayton was born in Mobile in south Alabama , but he [V: was] reared in Alexandria , Virginia .',\n",
       "   'tags': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B-V',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  {'verb': 'reared',\n",
       "   'description': 'Clayton was born in Mobile in south Alabama , but [ARG1: he] was [V: reared] [ARGM-LOC: in Alexandria , Virginia] .',\n",
       "   'tags': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B-ARG1',\n",
       "    'O',\n",
       "    'B-V',\n",
       "    'B-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'I-ARGM-LOC',\n",
       "    'O']}],\n",
       " 'words': ['Clayton',\n",
       "  'was',\n",
       "  'born',\n",
       "  'in',\n",
       "  'Mobile',\n",
       "  'in',\n",
       "  'south',\n",
       "  'Alabama',\n",
       "  ',',\n",
       "  'but',\n",
       "  'he',\n",
       "  'was',\n",
       "  'reared',\n",
       "  'in',\n",
       "  'Alexandria',\n",
       "  ',',\n",
       "  'Virginia',\n",
       "  '.']}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(sentence='\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kasturba', 'Road', 'is', 'a', 'street', 'in', 'Bangalore', ',', 'the', 'capital', 'of', 'Karnataka', ',', 'India', ',', 'which', 'is', 'connected', 'to', 'M', 'G', 'Road', 'to', 'the', 'north', 'and', 'J', 'C', 'Road', 'to', 'the', 'south', '.']\n",
      "['[CLS]', 'ka', '##st', '##ur', '##ba', 'road', 'is', 'a', 'street', 'in', 'bangalore', ',', 'the', 'capital', 'of', 'karnataka', ',', 'india', ',', 'which', 'is', 'connected', 'to', 'm', 'g', 'road', 'to', 'the', 'north', 'and', 'j', 'c', 'road', 'to', 'the', 'south', '.', '[SEP]']\n",
      "['Some', 'important', 'landmarks', 'situated', 'along', 'Kasturba', 'Road', 'are', 'Sree', 'Kanteerava', 'Stadium', ',', 'Kanteerava', 'Indoor', 'Stadium', ',', 'Cubbon', 'Park', ',', 'Government', 'Museum', ',', 'Venkatappa', 'Art', 'Gallery', ',', 'Visvesvaraya', 'Industrial', 'and', 'Technological', 'Museum', 'and', 'UB', 'City', '.']\n",
      "['[CLS]', 'some', 'important', 'landmarks', 'situated', 'along', 'ka', '##st', '##ur', '##ba', 'road', 'are', 'sr', '##ee', 'kant', '##eer', '##ava', 'stadium', ',', 'kant', '##eer', '##ava', 'indoor', 'stadium', ',', 'cub', '##bon', 'park', ',', 'government', 'museum', ',', 've', '##nka', '##ta', '##ppa', 'art', 'gallery', ',', 'vis', '##ves', '##vara', '##ya', 'industrial', 'and', 'technological', 'museum', 'and', 'u', '##b', 'city', '.', '[SEP]']\n",
      "['A', '600-year', '-', 'old', 'Ganesha', 'temple', 'is', 'also', 'situated', 'on', 'Kasturba', 'Road', '.']\n",
      "['[CLS]', 'a', '600', '-', 'year', '-', 'old', 'gan', '##esh', '##a', 'temple', 'is', 'also', 'situated', 'on', 'ka', '##st', '##ur', '##ba', 'road', '.', '[SEP]']\n",
      "['It', 'was', 'earlier', 'known', 'as', 'Sydney', 'Road', '.']\n",
      "['[CLS]', 'it', 'was', 'earlier', 'known', 'as', 'sydney', 'road', '.', '[SEP]']\n",
      "['Other', 'important', 'landmarks', 'close', 'to', 'the', 'road', 'are', 'Karnataka', 'High', 'Court', ',', 'Vidhana', 'Soudha', 'and', 'Chinnaswamy', 'Stadium', '.']\n",
      "['[CLS]', 'other', 'important', 'landmarks', 'close', 'to', 'the', 'road', 'are', 'karnataka', 'high', 'court', ',', 'vidhan', '##a', 'so', '##ud', '##ha', 'and', 'chin', '##nas', '##wa', '##my', 'stadium', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for ex_idx, d in enumerate(data[:1]):\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        example = du.preproc_text(para)\n",
    "        if len(example) < 500:\n",
    "            continue\n",
    "        doc = nlp(example)\n",
    "        for sent in doc.sents:\n",
    "            span = sent.text\n",
    "            lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "            srl_parse = predictor.predict(sentence=span)\n",
    "            token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "            word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: [1], 1: [2], 2: [3], 3: [4], 4: [5], 5: [6], 6: [7], 7: [8], 8: [9], 9: [10], 10: [11], 11: [12], 14: [18], 16: [23], 17: [24], 12: [13, 14], 13: [15, 16, 17], 15: [19, 20, 21, 22]})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_chars(toks):\n",
    "    count = 0\n",
    "    for t  in toks:\n",
    "        if t in ['[SEP]', '[CLS]']:\n",
    "            continue\n",
    "        count += len(t)\n",
    "        if t.startswith(\"##\"):\n",
    "            count -= 2\n",
    "    return count\n",
    "            \n",
    "def new_map_words_to_vectors(allen_srl_words, bert_tokenized_words):\n",
    "    word2veclist = defaultdict(list)\n",
    "    allen_srl_words = [w.lower() for w in allen_srl_words]\n",
    "    all_srl_start_map = {}\n",
    "    bert_tokenized_start_map = {}\n",
    "#     print(allen_srl_words)\n",
    "#     print(bert_tokenized_words)\n",
    "    for i, word in enumerate(allen_srl_words):\n",
    "        all_srl_start_map[count_chars(allen_srl_words[:i])] = (i, word)\n",
    "    for i, word in enumerate(bert_tokenized_words):\n",
    "        bert_tokenized_start_map[count_chars(bert_tokenized_words[:i])] = (i, word)\n",
    "    # align all tokens with the same char_count and string\n",
    "    for k, (srl_index, srl_word) in all_srl_start_map.items():\n",
    "        if k in bert_tokenized_start_map and bert_tokenized_start_map[k][1] == srl_word:\n",
    "            word2veclist[srl_index].append(bert_tokenized_start_map[k][0])\n",
    "    \n",
    "    for k, (srl_index, srl_word) in all_srl_start_map.items():\n",
    "        if srl_index in word2veclist or k not in bert_tokenized_start_map:\n",
    "            continue\n",
    "        # matching start char_count but terms are different\n",
    "        #print(srl_word, bert_tokenized_start_map[k][1])\n",
    "        word2veclist[srl_index].append(bert_tokenized_start_map[k][0])\n",
    "    completed_tokenized_indexes = set([v for vals in word2veclist.values() for v in vals])\n",
    "    inverse_word2veclist = {}\n",
    "    for k, v in word2veclist.items():\n",
    "        for vv in v:\n",
    "            inverse_word2veclist[vv] = k\n",
    "    for bert_tok_index, bert_tok_word in enumerate(bert_tokenized_words):\n",
    "        if bert_tok_index in completed_tokenized_indexes or bert_tok_word in ['[SEP]', '[CLS]']:\n",
    "            continue\n",
    "        # print(\"#\"*20)\n",
    "        # print(bert_tok_index, bert_tok_word)\n",
    "        # if token before this token is completed and word index after is in word2veclist\n",
    "        # just add this to the prev tok\n",
    "        if (bert_tok_index - 1) in completed_tokenized_indexes:\n",
    "#             print(\"Prev tok index is complete\")\n",
    "#             print(\"Prev Tok\", bert_tok_index - 1, bert_tokenized_words[bert_tok_index - 1])\n",
    "            srl_word_of_prev_tok = inverse_word2veclist[bert_tok_index - 1]\n",
    "            # print(\"SRL word of prev tok:\", allen_srl_words[srl_word_of_prev_tok])\n",
    "            if (srl_word_of_prev_tok + 1) in word2veclist or srl_word_of_prev_tok + 1 ==  len(allen_srl_words):\n",
    "                word2veclist[srl_word_of_prev_tok].append(bert_tok_index)\n",
    "                inverse_word2veclist[bert_tok_index] = srl_word_of_prev_tok\n",
    "                completed_tokenized_indexes.add(bert_tok_index)\n",
    "                # print(\"Adding \", bert_tokenized_words[bert_tok_index], \"to\", allen_srl_words[srl_word_of_prev_tok])\n",
    "    # At this point, the idea is that if there are still any srl_words not in the word2veclist\n",
    "    # if their neighbors are in, then you can just map all the tokens not used between those two\n",
    "    for srl_index, srl_word in enumerate(allen_srl_words):\n",
    "        if srl_index not in word2veclist:\n",
    "            if srl_index -1 in word2veclist and srl_index +1 in word2veclist:\n",
    "                start = max(word2veclist[srl_index -1])\n",
    "                end = min(word2veclist[srl_index +1])\n",
    "                tok_indexes = list(range(start+1, end))\n",
    "                if tok_indexes:\n",
    "                    word2veclist[srl_index].extend(tok_indexes)\n",
    "                else:\n",
    "                    word2veclist[srl_index] = []\n",
    "    \n",
    "    try:\n",
    "        if not len(word2veclist.keys()) == len(allen_srl_words):\n",
    "            raise Exception\n",
    "        if not len([v for vals in word2veclist.values() for v in vals]) == len(bert_tokenized_words) - 2:\n",
    "            raise Exception(\"Er mergahd\")\n",
    "    except Exception as e:\n",
    "        print(word2veclist)\n",
    "        print(all_srl_start_map)\n",
    "        print(bert_tokenized_start_map)\n",
    "        raise e\n",
    "    return  word2veclist\n",
    "    \n",
    "\n",
    "word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)\n",
    "print(word2veclist)\n",
    "assert len(word2veclist.keys()) == len(srl_parse['words'])\n",
    "assert len([v for vals in word2veclist.values() for v in vals]) == len(token_strs) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'ARTICLE_\\nFluor-liddicoatite\\n',\n",
       " 'SECTION_\\nCrystal habit\\n',\n",
       " 'PARAGRAPH_\\nCrystals are stout prismatic, with a curved convex trigonal outline, generally elongated and striated parallel to the c axis.  Crystals are hemimorphic, meaning that the two ends of the crystal have different forms.  Fluor-liddicoatite usually has a pedion (a single crystal face) opposite one or two pyramids.\\n',\n",
       " 'SECTION_\\nPhysical properties\\n',\n",
       " 'PARAGRAPH_\\nThe color is usually smoky brown, but also pink, red, green, blue, or rarely white.  Color zoning is abundant at the type locality, parallel to pyramid faces. This is due to changes in the solution during crystal growth. As the concentration of trace elements that serve as coloring agents changes, there will be areas of less or more color in different parts of the crystal. When the crystal is sliced perpendicular to the c axis, triangular zoning may be seen, together with a trigonal star that radiates from the centre of the crystal, with the three rays directed towards the corners of the triangular color patterns.  _NEWLINE_The pink-red color is due to the manganese Mn content, and the green color is due to intervalence charge transfer transactions between iron Fe and titanium Ti._NEWLINE__NEWLINE_The streak is white to very light brown, lighter than the mass color, luster is vitreous and crystals are transparent to translucent._NEWLINE_ _NEWLINE_Cleavage is poor perpendicular to the c crystal axis, or it may be totally absent. The mineral is brittle, with an  uneven to conchoidal fracture.  It is very hard, with hardness 7, a little harder than zircon, making it suitable for use as a gemstone.  Specific gravity is 3.02, a little lighter than fluorite.  It is neither fluorescent nor radioactive.\\n',\n",
       " 'SECTION_\\nOptical properties\\n',\n",
       " 'PARAGRAPH_\\nFluor-liddicoatite is uniaxial (-), with  _NEWLINE_refractive Indices N = 1.637 and N = 1.621 for the type specimen.  The refractive indices, however, will vary from specimen to specimen, as they depend on the content of iron and manganese, which are usually present as trace elements.Pleochroism is strong: O dark brown or pink, E light brown or pale pink.\\n',\n",
       " 'SECTION_\\nEnvironment\\n',\n",
       " 'PARAGRAPH_\\nFluor-liddicoatite is detrital in soil at the type locality, presumably derived from the weathering of granitic pegmatites.  Associated minerals are quartz, elbaite, albite and micas.']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[6]['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8').split(\"_START_\")# .decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[6]['text'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:discoher]",
   "language": "python",
   "name": "conda-env-discoher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
