{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: wiki40b/en\n",
      "INFO:absl:Load dataset info from gs://tfds-data/datasets/wiki40b/en/1.3.0\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset wiki40b (gs://tfds-data/datasets/wiki40b/en/1.3.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train[:5], from gs://tfds-data/datasets/wiki40b/en/1.3.0\n"
     ]
    }
   ],
   "source": [
    "data = du.get_wiki_data(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given 1 document\n",
    "\n",
    "* split doc into paragraphs\n",
    "* filter paragraph by length\n",
    "* encode each paragraph\n",
    "* serialize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nKasturba Road is a street in Bangalore, the capital of Karnataka, India, which is connected to M G Road to the north and J C Road to the south. Some important landmarks situated along Kasturba Road are Sree Kanteerava Stadium, Kanteerava Indoor Stadium, Cubbon Park, Government Museum, Venkatappa Art Gallery, Visvesvaraya Industrial and Technological Museum and UB City. A 600-year-old Ganesha temple is also situated on Kasturba Road.\\nIt was earlier known as Sydney Road.\\nOther important landmarks close to the road are Karnataka High Court, Vidhana Soudha and Chinnaswamy Stadium.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du.get_paragraphs(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = du.srl_paragraphs_batched(du.get_paragraphs(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cPickle\n",
    "du.serialize_reprs(du.srl_paragraphs_batched(du.get_paragraphs(data[1])), \"/tmp/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kasturba Road is a street in Bangalore, the capital of Karnataka, India, which is connected to M G Road to the north and J C Road to the south.', [[('V', [6]), ('ARG1', [1, 2, 3, 4, 5]), ('ARG2', [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35])], [('V', [20])], [('V', [21]), ('ARG1', [7, 8, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35])]])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/tmp/test.pkl\", \"rb\") as f:\n",
    "    unpickler = pickle.Unpickler(f)\n",
    "    test = unpickler.load() \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelize one worker\n",
    "* batchify over multiple paragraphs\n",
    "* write all outputs to single file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shard data\n",
    "* simply take a parameter for what slice of data to operate over\n",
    "* check all pylint bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "* time execution of 1 paragraph on cpu vs gpu\n",
    "* on best setup, optimize batch size\n",
    "* do calculation and choose appropriate sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for d in data:\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        proced_text = du.preproc_text(para)\n",
    "        if len(proced_text) < 500:\n",
    "            continue\n",
    "        paragraphs.append(proced_text)\n",
    "        # print(proced_text)\n",
    "        # print(len(proced_text))\n",
    "        \n",
    "        #print(\"~\"*20)\n",
    "    #print(\"#\"*20)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format\n",
    "* We need to keep both paragraph level and document level examples together.\n",
    "* We can train consecutively any paragraph but also we can train any document with multiple paragraphs\n",
    "\n",
    "## First try\n",
    "* Split by paragraph\n",
    "* Filter for paragraph length (above some length)\n",
    "* We get roughly 1 paragraph over 1000 characters per document. That's not great.\n",
    "* 789 / 500 docs with cutoff at 500 characters\n",
    "\n",
    "# TODO\n",
    "* Upgrade Spacy en_core_web_sm to latest\n",
    "* fix data encoder error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = du.encode_data_srl_only(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_srl_only(data):\n",
    "    nlp = du.create_nlp()\n",
    "    tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    predictor = du.make_allennlp()\n",
    "    reprs = []\n",
    "    for ex_idx, d in enumerate(data):\n",
    "        text = d['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8')\n",
    "        for para in text.split(\"_START_\"):\n",
    "            if not para.startswith(\"PARAGRAPH_\"):\n",
    "                continue\n",
    "            example = du.preproc_text(para)\n",
    "            if len(example) < 500:\n",
    "                continue\n",
    "            doc = nlp(example)\n",
    "            for sent in doc.sents:\n",
    "                span = sent.text\n",
    "                lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "                srl_parse = predictor.predict(sentence=span)\n",
    "                token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "                try:\n",
    "                    word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)\n",
    "                except du.NoStopException as e:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(\"Index\", ex_idx)\n",
    "                    print(e)\n",
    "                    print(token_strs)\n",
    "                    print(srl_parse['words'])\n",
    "                    raise e\n",
    "                sent_repr = build_repr(srl_parse['verbs'], word2veclist)\n",
    "                reprs.append((span, sent_repr))\n",
    "    return reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = encode_data_srl_only(data[:2])\n",
    "print(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "span = '\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    "lm_tokenized_input = tokenizer(span, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix parsing errors\n",
    "* fix parser\n",
    "* update spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.get_wiki_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = encode_data_srl_only(data[27:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = du.make_allennlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.predict(sentence='\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_idx, d in enumerate(data[:1]):\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        example = du.preproc_text(para)\n",
    "        if len(example) < 500:\n",
    "            continue\n",
    "        doc = nlp(example)\n",
    "        for sent in doc.sents:\n",
    "            span = sent.text\n",
    "            lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "            srl_parse = predictor.predict(sentence=span)\n",
    "            token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "            word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[6]['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8').split(\"_START_\")# .decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[6]['text'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:discoher]",
   "language": "python",
   "name": "conda-env-discoher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
