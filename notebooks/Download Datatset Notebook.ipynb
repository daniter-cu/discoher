{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_utils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: wiki40b/en\n",
      "INFO:absl:Load dataset info from gs://tfds-data/datasets/wiki40b/en/1.3.0\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset wiki40b (gs://tfds-data/datasets/wiki40b/en/1.3.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split train[:5], from gs://tfds-data/datasets/wiki40b/en/1.3.0\n"
     ]
    }
   ],
   "source": [
    "data = du.get_wiki_data(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given 1 document\n",
    "\n",
    "* split doc into paragraphs\n",
    "* filter paragraph by length\n",
    "* encode each paragraph\n",
    "* serialize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nKasturba Road is a street in Bangalore, the capital of Karnataka, India, which is connected to M G Road to the north and J C Road to the south. Some important landmarks situated along Kasturba Road are Sree Kanteerava Stadium, Kanteerava Indoor Stadium, Cubbon Park, Government Museum, Venkatappa Art Gallery, Visvesvaraya Industrial and Technological Museum and UB City. A 600-year-old Ganesha temple is also situated on Kasturba Road.\\nIt was earlier known as Sydney Road.\\nOther important landmarks close to the road are Karnataka High Court, Vidhana Soudha and Chinnaswamy Stadium.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du.get_paragraphs(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = du.srl_paragraphs_batched(du.get_paragraphs(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cPickle\n",
    "du.serialize_reprs(du.srl_paragraphs_batched(du.get_paragraphs(data[1])), \"/tmp/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kasturba Road is a street in Bangalore, the capital of Karnataka, India, which is connected to M G Road to the north and J C Road to the south.', [[('V', [6]), ('ARG1', [1, 2, 3, 4, 5]), ('ARG2', [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35])], [('V', [20])], [('V', [21]), ('ARG1', [7, 8, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35])]])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/tmp/test.pkl\", \"rb\") as f:\n",
    "    unpickler = pickle.Unpickler(f)\n",
    "    test = unpickler.load() \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelize one worker\n",
    "* batchify over multiple paragraphs\n",
    "* write all outputs to single file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shard data\n",
    "* simply take a parameter for what slice of data to operate over\n",
    "* check all pylint bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "* time execution of 1 paragraph on cpu vs gpu\n",
    "* on best setup, optimize batch size\n",
    "* do calculation and choose appropriate sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for d in data:\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        proced_text = du.preproc_text(para)\n",
    "        if len(proced_text) < 500:\n",
    "            continue\n",
    "        paragraphs.append(proced_text)\n",
    "        # print(proced_text)\n",
    "        # print(len(proced_text))\n",
    "        \n",
    "        #print(\"~\"*20)\n",
    "    #print(\"#\"*20)\n",
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paragraphs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format\n",
    "* We need to keep both paragraph level and document level examples together.\n",
    "* We can train consecutively any paragraph but also we can train any document with multiple paragraphs\n",
    "\n",
    "## First try\n",
    "* Split by paragraph\n",
    "* Filter for paragraph length (above some length)\n",
    "* We get roughly 1 paragraph over 1000 characters per document. That's not great.\n",
    "* 789 / 500 docs with cutoff at 500 characters\n",
    "\n",
    "# TODO\n",
    "* Upgrade Spacy en_core_web_sm to latest\n",
    "* fix data encoder error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = du.encode_data_srl_only(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_srl_only(data):\n",
    "    nlp = du.create_nlp()\n",
    "    tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    predictor = du.make_allennlp()\n",
    "    reprs = []\n",
    "    for ex_idx, d in enumerate(data):\n",
    "        text = d['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8')\n",
    "        for para in text.split(\"_START_\"):\n",
    "            if not para.startswith(\"PARAGRAPH_\"):\n",
    "                continue\n",
    "            example = du.preproc_text(para)\n",
    "            if len(example) < 500:\n",
    "                continue\n",
    "            doc = nlp(example)\n",
    "            for sent in doc.sents:\n",
    "                span = sent.text\n",
    "                lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "                srl_parse = predictor.predict(sentence=span)\n",
    "                token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "                try:\n",
    "                    word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)\n",
    "                except du.NoStopException as e:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(\"Index\", ex_idx)\n",
    "                    print(e)\n",
    "                    print(token_strs)\n",
    "                    print(srl_parse['words'])\n",
    "                    raise e\n",
    "                sent_repr = build_repr(srl_parse['verbs'], word2veclist)\n",
    "                reprs.append((span, sent_repr))\n",
    "    return reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = encode_data_srl_only(data[:2])\n",
    "print(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = du.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "span = '\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    "lm_tokenized_input = tokenizer(span, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix parsing errors\n",
    "* fix parser\n",
    "* update spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.get_wiki_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs = encode_data_srl_only(data[27:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprs[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = du.make_allennlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.predict(sentence='\\nClayton was born in Mobile in south Alabama, but he was reared in Alexandria, Virginia.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_idx, d in enumerate(data[:1]):\n",
    "    text = d['text'].numpy().decode('utf-8')\n",
    "    for para in text.split(\"_START_\"):\n",
    "        if not para.startswith(\"PARAGRAPH_\"):\n",
    "            continue\n",
    "        example = du.preproc_text(para)\n",
    "        if len(example) < 500:\n",
    "            continue\n",
    "        doc = nlp(example)\n",
    "        for sent in doc.sents:\n",
    "            span = sent.text\n",
    "            lm_tokenized_input = tokenizer(span, return_tensors='pt')\n",
    "            srl_parse = predictor.predict(sentence=span)\n",
    "            token_strs = tokenizer.convert_ids_to_tokens(lm_tokenized_input['input_ids'].numpy()[0])\n",
    "            word2veclist = new_map_words_to_vectors(srl_parse['words'], token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[6]['text'].numpy().decode('UTF-8').encode('ascii', 'ignore').decode('UTF-8').split(\"_START_\")# .decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[6]['text'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_str = \"\"\"\n",
    "The domains wikipedia.com and wikipedia.org were registered on January 12, 2001,[22] and January 13, 2001,[23] respectively, and Wikipedia was launched on January 15, 2001,[14] as a single English-language edition at www.wikipedia.com,[24] and announced by Sanger on the Nupedia mailing list.[18] Wikipedia's policy of \"neutral point-of-view\"[25] was codified in its first few months. Otherwise, there were relatively few rules initially and Wikipedia operated independently of Nupedia.[18] Originally, Bomis intended to make Wikipedia a business for profit.[26]\n",
    "\n",
    "\n",
    "The Wikipedia home page on December 17, 2001\n",
    "Wikipedia gained early contributors from Nupedia, Slashdot postings, and web search engine indexing. Language editions were also created, with a total of 161 by the end of 2004.[27] Nupedia and Wikipedia coexisted until the former's servers were taken down permanently in 2003, and its text was incorporated into Wikipedia. The English Wikipedia passed the mark of two million articles on September 9, 2007, making it the largest encyclopedia ever assembled, surpassing the Yongle Encyclopedia made during the Ming Dynasty in 1408, which had held the record for almost 600 years.[28]\n",
    "\n",
    "Citing fears of commercial advertising and lack of control in Wikipedia, users of the Spanish Wikipedia forked from Wikipedia to create the Enciclopedia Libre in February 2002.[29] Wales then announced that Wikipedia would not display advertisements, and changed Wikipedia's domain from wikipedia.com to wikipedia.org.[30][31]\n",
    "\n",
    "Though the English Wikipedia reached three million articles in August 2009, the growth of the edition, in terms of the numbers of new articles and of contributors, appears to have peaked around early 2007.[32] Around 1,800 articles were added daily to the encyclopedia in 2006; by 2013 that average was roughly 800.[33] A team at the Palo Alto Research Center attributed this slowing of growth to the project's increasing exclusivity and resistance to change.[34] Others suggest that the growth is flattening naturally because articles that could be called \"low-hanging fruit\"—topics that clearly merit an article—have already been created and built up extensively.[35][36][37]\n",
    "\n",
    "\n",
    "File:Wikipedia Edit 2014.webm\n",
    "A promotional video of the Wikimedia Foundation that encourages viewers to edit Wikipedia, mostly reviewing 2014 via Wikipedia content\n",
    "In November 2009, a researcher at the Rey Juan Carlos University in Madrid found that the English Wikipedia had lost 49,000 editors during the first three months of 2009; in comparison, the project lost only 4,900 editors during the same period in 2008.[38][39] The Wall Street Journal cited the array of rules applied to editing and disputes related to such content among the reasons for this trend.[40] Wales disputed these claims in 2009, denying the decline and questioning the methodology of the study.[41] Two years later, in 2011, Wales acknowledged the presence of a slight decline, noting a decrease from \"a little more than 36,000 writers\" in June 2010 to 35,800 in June 2011. In the same interview, Wales also claimed the number of editors was \"stable and sustainable\".[42] A 2013 article titled \"The Decline of Wikipedia\" in MIT Technology Review questioned this claim. The article revealed that since 2007, Wikipedia had lost a third of its volunteer editors, and those still there have focused increasingly on minutiae.[43] In July 2012, The Atlantic reported that the number of administrators is also in decline.[44] In the November 25, 2013, issue of New York magazine, Katherine Ward stated \"Wikipedia, the sixth-most-used website, is facing an internal crisis\".[\n",
    "\"\"\"\n",
    "x = tokenizer(long_str, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "789"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:discoher]",
   "language": "python",
   "name": "conda-env-discoher-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
